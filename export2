# === CONFIG ===
TENANT_ID     = "<tenant-guid>"
CLIENT_ID     = "<client-id>"
CLIENT_SECRET = "<client-secret>"

DAY_UTC       = "2025-09-01"         # UTC day to export (YYYY-MM-DD)
SLICE_MINUTES = 1                    # time slice length (1â€“5 recommended for big tenants)
PAGE_SIZE     = 100000               # AH max rows per page
OUT_DIR       = "out_dne_2025-09-01" # output folder for NDJSON files

# === SCRIPT ===
import os, time, json, datetime, typing
import requests

TOKEN_URL = f"https://login.microsoftonline.com/{TENANT_ID}/oauth2/v2.0/token"
GRAPH_SCOPE = "https://graph.microsoft.com/.default"
RUN_URL = "https://graph.microsoft.com/v1.0/security/runHuntingQuery"

os.makedirs(OUT_DIR, exist_ok=True)

def get_token() -> str:
    data = {
        "client_id": CLIENT_ID,
        "client_secret": CLIENT_SECRET,
        "scope": GRAPH_SCOPE,
        "grant_type": "client_credentials",
    }
    r = requests.post(TOKEN_URL, data=data, timeout=30)
    r.raise_for_status()
    return r.json()["access_token"]

def run_query(token: str, kql: str) -> dict:
    headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}
    payload = {"Query": kql}
    backoff = 2
    while True:
        resp = requests.post(RUN_URL, headers=headers, json=payload, timeout=175)  # < 3 min API limit
        if resp.status_code in (429, 500, 502, 503, 504):
            wait = int(resp.headers.get("Retry-After", backoff))
            time.sleep(min(wait, 30))
            backoff = min(backoff * 2, 60)
            continue
        resp.raise_for_status()
        return resp.json()

def build_kql(slice_start: datetime.datetime,
              slice_end: datetime.datetime,
              last_ts: typing.Optional[str],
              last_report_id: typing.Optional[str]) -> str:
    s = slice_start.strftime("%Y-%m-%dT%H:%M:%SZ")
    e = slice_end.strftime("%Y-%m-%dT%H:%M:%SZ")
    lines = []
    lines.append(f"let SliceStart=datetime({s});")
    lines.append(f"let SliceEnd=datetime({e});")
    lines.append(f"let PageSize={PAGE_SIZE};")
    lines.append("DeviceNetworkEvents")
    lines.append("| where Timestamp between (SliceStart..SliceEnd)")
    if last_ts is not None and last_report_id is not None:
        lines.append(f"| where Timestamp>datetime({last_ts}) or (Timestamp==datetime({last_ts}) and ReportId>\"{last_report_id}\")")
    lines.append("| order by Timestamp asc,ReportId asc")
    lines.append("| take PageSize")
    return "\n".join(lines)

def rows_from_graph(result: dict):
    """
    Graph returns:
    {
      "Results": [
        {"TableName":"PrimaryResult","Columns":[...],"Rows":[...]}
      ]
    }
    """
    tables = result.get("Results", [])
    if not tables:
        return []
    t = tables[0]
    cols = [c["Name"] for c in t.get("Columns", [])]
    out = []
    for r in t.get("Rows", []):
        obj = {cols[i]: r[i] for i in range(len(cols))}
        out.append(obj)
    return out

def drain_slice(token: str, slice_start: datetime.datetime, slice_end: datetime.datetime) -> int:
    page = 1
    total = 0
    last_ts = None
    last_rid = None
    while True:
        kql = build_kql(slice_start, slice_end, last_ts, last_rid)
        data = run_query(token, kql)
        results = rows_from_graph(data)
        if not results:
            break
        tag = f"DNE_{slice_start.strftime('%Y-%m-%dT%H-%M')}_p{page}"
        path = os.path.join(OUT_DIR, f"{tag}.ndjson")
        with open(path, "w", encoding="utf-8") as f:
            for row in results:
                f.write(json.dumps(row, ensure_ascii=False) + "\n")
        total += len(results)
        last = results[-1]
        last_ts = last["Timestamp"]
        last_rid = str(last["ReportId"])
        page += 1
        if len(results) < PAGE_SIZE:
            break
    return total

def iter_day(day_utc: str, minutes: int):
    start = datetime.datetime.fromisoformat(day_utc + "T00:00:00+00:00").astimezone(datetime.timezone.utc)
    end = start + datetime.timedelta(days=1)
    cur = start
    step = datetime.timedelta(minutes=minutes)
    while cur < end:
        nxt = min(cur + step, end)
        yield cur, nxt
        cur = nxt

def main():
    token = get_token()
    total = 0
    for s, e in iter_day(DAY_UTC, SLICE_MINUTES):
        total += drain_slice(token, s, e)
        time.sleep(0.2)  # gentle pacing
    print(f"Done. Rows exported: {total}. Files in: {OUT_DIR}")

if __name__ == "__main__":
    main()
